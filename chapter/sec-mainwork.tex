\chapter{Draw with Me} \label{sec-mainwork}

		In this chapter, we introduce our facial drawing assistant named  \textit{Draw with Me}  in the following four sections. First we describe the system UI and basic operations to show how it works for users. Second we explain visual guides provided by the system, which is the main technical contribution of our work, and how to generate such accurate guidance by introducing our feature enhancement algorithms. In the third section we talk about the canvas, which mimics traditional pencil sketching and provided more efficient operations for learners through practicing their drawings. In the fourth section we explain how to coordinate the interactions during the teaching and practicing process, in a manner of state transformation with the help of the classic DFA (deterministic finite automaton) model.

\section{User Interface}

		\textit{Draw with Me} consists of three basic logical components, the tutor, the canvas and the DFA guided interaction. In the following, we will elaborate them one by one. 
	
		\begin{figure}[htbp]
			\centering
				\includegraphics[width=0.70\textwidth]{D:/documents/ust2013/UI_1.JPG}
			\caption{User Interface Overview}
			\label{fig:UI}
		\end{figure}
		
		%	UI Overview
		
		Figure \ref{fig:UI} shows our main user interface is composed of two side-by-side panels. On the left is tutor panel and on the right is canvas panel. Tutor panel is used to display the selected face photo, and more importantly, to generate all kinds of visual guides on top of the photo after the learners start to draw. It is designed as our ''AI teacher'' by quantifying the human tutor's instructions into procedures the computer can understand. By providing well designed visual guides, which will be elaborated on section \ref{sec-vg}, the system does a good imitation of a human teacher. Canvas panel takes the place of physical sketching paper. It is responsible to record and display every stroke drawn by the user, either using a digital pen or a finger on a touch screen, or a mouse. The displaying strokes on the canvas have been textured in advance to mimic pencil strokes. 
		
		When we design the user interface, we hope users of our system can focus on the teaching and practicing scenario, instead of being interrupted by complicated interactions such as menu selections, button clicks etc. So the design principle is set to be as simple as possible, meanwhile simulating the physical drawing process as closely as possible. Therefore, we provide concise interface without too many buttons or sub-menus. The canvas and pencil are closest to physical reality other than operations like undo stroke for efficiency.
			
		%	UI Operations
		
		We introduce the basic operations by describing a \textbf{typical} scenario of using our system. After starting up \textit{Draw with Me}, users are required to select the face to draw by simply clicking ''Load Image'' on the menu bar. A navigation window will show up for users to choose a preferred portrait photo file. After that our system will pass the input photo to third-party  facial detection and landmark extraction APIs to get the raw facial feature points and do some clipping to guarantee that the facial region in the photo is fully displayed. Hereafter users can see the displayed portrait photo in the tutor panel, meanwhile the system does feature enhancement and guides generation work in the backstage. Then a six-square auxiliary box appears in the tutor panel and our system starts to teach. By following the six-square guide, it's very clear for users to see the location relations between facial facial features and the differences between square boundary lines and facial features' distribution (Outer boundary lines of six-square shows the difference to the skull while inner lines shows the differences to the facial features). We call these differences good disparities as they reveal the geometric identity of the face, on which we rely to draw the head skull. The tutor panel waits for user's drawing on the canvas before it moves on to provide another guide. Next the tutor will generate a pair of horizontal and a pair of vertical dot lines to further indicate the proportions and approximate positions of facial features: eyes, nose and the mouth. Users can draw the outline of each facial feature on the canvas based on the dot lines' auxiliary. The above two tutoring guides make users able to draw the face in a very coarse level by carefully observing locations, boundary positions as well as proportions. After that, the tutor panel will generate guides for each facial feature. Zoom options are provided by the tutor, as some facial feature details may not be easily captured in the global view. The tutoring order will be eyes, nose and mouth. Each time users learn to draw a facial feature, the tutor only highlights the region of interest (ROI) and darken the others. The drawing order is indicated by the shift of highlighted ROIs. In the whole drawing process, users can add and undo strokes on canvas easily, without pollution caused by physical eraser. Users can save their drawings anytime reload to continue drawing easily.
		
		A collection of visual guides are displayed in Figure ?\\
		\begin{center}
			insert visual guides fig table here
			\end{center}
		
		% \subsection{Work Flow}
		Chart ? shows the typical working flow of \textit{Draw with Me}, namely from user choose a photo to draw, feature points extraction, feature enhancement to teaching and practicing interaction. \\
			\begin{center}
			insert flow chart here
			\end{center}

		According to Chart ?, some special statuses are stated here as a supplement. The first case is when users suddenly stop the tutoring process by trigger the ''Load Image'' menu again, our system will notify by a dialog window for users to choose whether continue to choose another photo or back to save the current work. Another case is we do set timers to record the practicing period, thus a notice dialog will show up when time is over to let users choose what to do, continuing awaiting for the current stage or jump to the next one. So a dead waiting will never take place. One more case is when users do not practice by following the tutor's orders, for example, maybe currently the nose guides are provided by the tutor but users are drawing in the area which is related to the mouth according to our canvas's records, \textit{Draw with Me} will also prompt a dialog window for users to choose terminate tutor's instructions to draw by themselves or continue the tutorial triggered by users correct drawing on the nose related region.

\section{Feature Enhancement} \label{sec-vg}
		In order to replace human teachers, the tutor panel provides thorough non-textual instructions by utilizing various vision cues such as highlighted regions, scaffolding lines, reckon by time and sketch animation. These visual guides cannot be so accurate and powerful without our enhanced features. Figure ? shows the raw feature points collected by our system. From which we can see that there is no feature points in the outer eyelid and flaws and inaccuracy points can also be found in the state of the art face detection techniques. To our knowledge, lack of details problem is because face detection is a general technique for global facial landmarks. Detailed eyelids are not in their current concern. Besides, mismatching problem are lying in the intrinsic of these learning based techniques, which cannot handle details the way as local optimizing algorithms do. \\
		
		\begin{center}
				insert raw feature points fig here
		\end{center}
		
		Based on the above observation, we would like to solve the facial feature points inaccuracy and deficiency (for outer eyelids) problems with the help of EZ-Sketching. EZ-Sketching is a sketch refinement tool which considers three-level optimization at the same time. We solve the feature inaccuracy problem by using EZ-Sketching's local optimization algorithm and work out the feature deficiency problem by using EZ-Sketching's local and semi-global optimizations simultaneously. Let us elaborate accordingly as follows.
		
		We modified EZ-Sketching's interface to allow directly passing strokes into the optimizer instead of from touch screen inputs. By connecting respective facial feature points into a ''feature stroke'', we can adopt local optimizer to refine the positions of mismatching points back to the underlying image features. Thus the refined feature stroke is as accurate as possible with the respective local image feature. Figure ? shows some of the refined feature points from chin contour or lower eyelids.
		
		\begin{center}
				insert fig to show local ez refined feature points here
		\end{center}
		
		To solve the feature deficiency problem, we need to augment corresponding feature points for outer eyelids. First we compute a set of artificial points for each outer eyelid as the initial feature points, based on the geometry relations illustrated in Figure ? By simply passing the initial points into EZ-Sketching's local optimizer cannot fix this problem due to the closeness and overlaps between the outer eyelids and inner ones. The same circumstance occured when we switch to other local optimization based algorithms. So we brainstorm to use EZ-Sketching local and semi-global optimiztion at the same time to optimize both outer eyelids and inner eyelids, and Figure ? shows that we achieved very exciting results.
		
		\begin{center}
				insert fig to show initial feature points for outer eyelid
		\end{center}

		\begin{center}
				insert fig to show augmented outer eyelids
		\end{center}

\section{DFA guided Interaction}

		%\subsection{Top Level Guidance}
		%
				%facial feature distribution, geometric relations, drawing order etc.
						%
		%\subsection{facial feature-wise guidance}
				%show eye, nose, mouth e.g. here 
				%
		%\subsection{Implementation}
			%
				%facial feature points collection \\
					%
				%feature enhancement \\
					%
						%a. intro 2 ez-skecthing \\
					%
						%b. augmentation, e.g. outer eye lids \\
					%
						%c. refinement, e.g. chin contour, inner eye lids, (mouth lips still not work) \\
							%
						%d. from points to curved lines \\
%
			%
							%
%\section{Drawing on Canvas}
	%
		%\subsection{Pencil Style visualization}
			%
		%\subsection{Productivity Tools}
					%save, load, withdraw user sketches
			%
					%
					%
%\section{Interaction between above two}
%
		%\subsection{Intro 2 DFA}
		%
		%\subsection{DFA flow chart in this system}



%--------------------------------------------------------------------------------------------------------------------------------------------------------------------
%In this chapter, we present our design for Mars, with emphasis on the GPU-based component.
%Our design is guided by the following three goals.
%
%\begin{enumerate}
  %\item {\em Programmability}. User code size reduction encourages programmers to use the GPU for their tasks.
  %\item {\em Flexibility}. The design should be applicable to various multi-/many-core processors, e.g., multi-core CPUs and AMD GPUs, and should be as expressive as the underlying runtime, e.g., NVIDIA CUDA, AMD Brook+ or pthreads, so that the system will work for a wide range of hardware and applications.
  %\item {\em High Performance}. The overall performance should be accelerated by the GPU effectively.
%\end{enumerate}
%
%\section{Overview}
%\red{By examining the Phoenix design, we see that there are three potential sources of overhead.  First, the tight-coupling of the Map and Reduce stages makes every application go through both stages, no matter whether they need both stages or not. Second, a dynamic thread scheduler for task assignment heavily relies on locking to implement synchronization. Third, each reduce worker may require frequent data movement for sorting the static output array, and the data movement can become a bottleneck for the overall performance. The latest paper about Phoenix also points out this problem~\cite{Yoo2009}.}
%
%
%In the Mars design, we decide to separate a MapReduce workflow into three loosely coupled stages -
%{\em Map}, {\em Group} and {\em Reduce}. The {\em Group} stage is designed to group {\em Map} output by key, which is the format for {\em Reduce} input. Our observation is that some applications need only the
%{\em Map} stage, some need both {\em Map} and {\em Group}, and some need all of the three stages.
%\red{The Group stage is the same as running Reduce with the identity function in the original MapReduce system \cite{Dean2008}.
%Our purpose of providing an explicit Group stage is to allow a MapReduce application with high flexibility to customize its workflow,
%and to avoid the overhead of entering unnecessary stages. }
%No matter what configuration of the three stages is for an application, the MapReduce interface
%of Mars is unchanged - users write Map and/or Reduce functions when necessary.
%
%Moreover, we decide to use a lock-free scheme for synchronization
%and to perform in-advance buffer allocation. One reason is to
%avoid heavy overheads of locking and buffer reallocation. The
%other reason is that, current GPUs do not support locking
%or in-flight buffer reallocation. In our design, we statically
%distributes tasks to a massive number of GPU threads, so that we
%can fully utilize the parallelism of the GPU. We adopt a two-phase,
%lock-free scheme for result output. The basic idea is that, in the
%first phase, we calculate histograms on the size of output results for each
%thread, followed by a prefix sum operation on the histograms,
%so that we obtain both the exact output buffer size and the deterministic
%write position for each thread; in the second phase, we
%perform the actual computation and output. We will detail this strategy in
%Section~\ref{sec-lockfree}.
%
%\section{Data Structure} \label{sec-datastructure}
%Data structures in Mars affect the workflow, memory access patterns, and the expressiveness of the system.
%
%Since the GPU does not support dynamic memory allocation on the
%device memory during the execution of the GPU code, this limitation rules out
 %dynamic data structures such as queues and linked lists, as used in
%other MapReduce implementations. Instead, we use plain arrays as the
%main data structure in Mars. The {\em Map} stage takes input records in
%the key/value form, and outputs intermediate result records, which
%are in turn the input of the {\em Group} stage. The output of the {\em Group}
%stage is the input of the {\em Reduce} stage, and {\em Reduce} produces final output
%records. Each of the three sets - the input records, the intermediate records and the
%output records  - is stored in three arrays, i.e., the
%key array, the value array and the directory index array. The
%directory index consists of an entry of $<$key offset, key size,
%value offset, value size$>$ for each key/value pair. Given a
%directory index entry, we fetch the key or the value at the
%corresponding offset in the key array or the value array.
%
%Variable-sized types, such as strings, are supported with the directory index, since current GPUs have no such build-in types yet. If
%two key/value pairs need to be swapped, we swap their corresponding
%entries in the directory index without modifying the key and the
%value arrays.
%
%Some applications perform chained MapReduce procedures,
%where the output of one MapReduce procedure is the input of
%another one. Since the sets of
%input records, intermediate records and output records  are all
%in the three-array structure uniformly, chained MapReduce is
%supported gracefully in Mars.
%
%\section{Mars Workflow}
%Figure \ref{fig:MarsWorkflow} illustrates the workflow of Mars, assuming the data resides in the disk at the beginning.
%The Mars scheduler runs on the CPU, and schedules tasks to the GPU.
%Mars has three stages, {\em Map}, {\em Group}, and {\em Reduce}.
%
%\begin{figure}[ht]
  %\centering
  %\includegraphics[width=0.60\textwidth]{figure/Mars_workflow.eps}
  %\caption{The workflow of Mars on the GPU. }\label{fig:MarsWorkflow}
%\end{figure}
%
%Before the {\em Map} stage, Mars preprocesses on the CPU the input data from disk,
%transforming the input data to key/value pairs (input records) in main memory.
%After that, it transfers input records from the main memory to the GPU device memory.
%
%In the {\em Map} stage, {\em Map Split} dispatches
%input records to GPU threads such that the workload for all threads is even.
%Each thread executes the user-defined MapCount function to compute a local histogram on
%the number and the total size of intermediate records that {\em Map} will output.
%Then, the runtime performs a GPU-based {\em Prefix Sum} on the local histograms to
%obtain the output size and the write position for each thread.
%Finally, after the CPU allocates the output buffer in the device memory, each GPU
%thread executes the user-defined Map function and outputs results.  Since the write
%position for each thread is pre-computed and has no conflict with any other threads,
%there will be no write conflict between concurrent threads. This lock-free scheme of MapCount,
%Prefix Sum, and Map is adapted from our previous
%work~\cite{He2008a}.
%
%\red{In the {\em Group} stage, both sort-based and hash-based approaches are available for grouping records by key. However, we adopt the sort-based, because some applications require to sort all output records, and the hash-based approach has to perform additional sort within each hash bucket. }
%
%
%In the {\em Reduce} stage, {\em Reduce Split} dispatches each group of records
%with the same key to a GPU thread.
%However, it may cause load imbalance between threads, since the number of records of different groups may vary widely.
%We adopt a skew handling scheme to alleviate the load imbalance problem (Section~\ref{sec-reduce}).
%The {\em Reduce} stage then works in a lock-free scheme, similar to that in {\em Map}, to obtain the result size and the write location for each thread.
%Finally, all Reduce workers output results to a single buffer.
%
%Because these three stages are loosely coupled and not every application requires all
%stages, Mars allows users to customize the following workflows in their applications:
%\begin{itemize}
%\item MAP\_ONLY. Mars executes the {\em Map} stage only, and does not execute the {\em Group} or {\em Reduce} stage.
%\item MAP\_GROUP. Mars executes the {\em Map} and {\em Group} stages, and does not execute the {\em Reduce} stage.
%\item MAP\_GROUP\_REDUCE.  Mars executes all three stages -- {\em Map}, {\em Group}, and {\em Reduce} stages.
%\end{itemize}
%
%Because usually applications need a Map to transform input records, and a Group to
%prepare for the intermediate records to feed to Reduce, we exclude the other
%workflow configurations that skip either Map or Group in the presence of Reduce.
%
%\section{Lock-free Scheme} \label{sec-lockfree}
%
%With the array structure, we allocate the space on the device memory
%for the input data as well as for the result output before executing
%the GPU program. However, the sizes of the output from the {\em Map} and
%the {\em Reduce} stages are unknown. Moreover, write conflicts occur when
%multiple threads write results to the shared output array. To
%address these two problems, we adopt a previous lock-free output
%scheme for relational joins~\cite{He2008a}. Since the output scheme
%for the {\em Map} stage is similar to that for the {\em Reduce} stage, we
%present the scheme for the {\em Map} stage only.
%
%First, each MapCount invocation on a thread outputs three counts, i.e.,
%the number of intermediate results, the total size of intermediate keys (in bytes)
%and the total size of intermediate values (in bytes).
%Based on intermediate key sizes (or value sizes),
%Mars computes a prefix sum on these sizes and produces an array of write locations.
%A write location is the start location in the output array for a map task to write.
%Based on the number of intermediate results,
%Mars computes a prefix sum and produces an array of start locations in the output
%directory index.
%Through these prefix sums, we also know the sizes of the arrays for the intermediate
%results.
%Finally, Mars allocates arrays in the device memory with the exact sizes for
%storing the intermediate results.
%
%Second, each Map invocation on a thread outputs the intermediate key/value pairs to
%the output array.
%Since each Map has its deterministic and non-overlapping positions to write to,
%the write conflicts are avoided.
%
%
%The lock-free scheme is suitable for the massive thread parallelism on the GPU, even
%though it performs a MapCount in addition to a Map. The overhead of executing MapCount
%is application dependent, and is usually small. For example, this overhead is negligible
%in the matrix multiplication in our study, since MapCount simply emits the size without
%performing the actual multiplication. In addition, the code for MapCount function is also application dependent, while in most cases, programmers write one statement to emits output data sizes. 
%
%\red{
%\section{Rapid Group} \label{sec-rapid}
%The Group stage requires to sort intermediate records.
%However, we observe that some applications inherently have their intermediate records grouped after the Map phase, and each group has the same number of records.
%For example, [A,A,A,B,B,B,C,C,C] shows three groups with A, B, and C as the key respectively, and each group is with the same size 3.
%For such applications, Mars provides a configuration parameter for users to indicate whether the intermediate data is already grouped. The runtime automatically skips the time consuming sorting, and then dispatches each group of intermediate records with the same group size to Reduce workers.
%We name this strategy as ``Rapid Group".
%}
%
%\section{Skew handling} \label{sec-reduce}
%
%We design a skew handling scheme to distribute workloads evenly across reduce workers, where the user-defined Reduce operation is commutative and associative.
%This scheme iteratively performs the {\em Reduce} stage in the following two steps.  First, we divide the data into $M$ equal-sized chunks.  Second, we perform a reduction on each chunk. In this step, each of the $M$ threads applies the reduce function on groups of records in a single chunk.  Note, in each iteration, we perform reduction on the intermediate results with the same keys only.
%
%%This scheme starts within the {\em Group} stage, immediately after the sorting, and performs in the following three steps.  First, we divide the data into $M$ equal-sized chunks. Second, we perform a reduction on each chunk.  In this step, each of the $M$ threads applies the reduce function on groups of records in a single chunk, called {\em local reduction}.  Finally, we group the reduced intermediate results and pass them to the {\em Reduce} stage.  This skew handling scheme makes sure that {\em local reduction} in {\em Group} stage are load-balanced.  Additionally, the {\em Reduce} stage processes compact groups of records, so that it alleviates the bottleneck caused by the largest group.
%
%\section{Mars APIs}
%Mars provides a small set of APIs. Similar to the existing MapReduce
%frameworks, Mars has two kinds of APIs, the user-implemented APIs,
%which the users should implement by themselves, and the
%system-provided APIs, which the users can use as library calls.
%%We refer the readers to our conference paper~\cite{He2008} for more details about Mars APIs.
%The definitions of these APIs are in Table \ref{tb:marsapi}.
%
%\doublerulesep 0.1pt
%\begin{table}[htb]
  %\centering
 %\linespread{1.7}{ {\footnotesize
  %\caption{Mars APIs}\label{tb:marsapi}
%\vspace{2em}
  %\begin{tabular}{p{5cm}p{7.5cm}p{3.5cm}}
  %\hline
%\noalign{\smallskip}
   %\textbf{Function Name} & \textbf{Description} & \textbf{Function Type}\\
%\noalign{\smallskip}
  %\hline
   %MAP\_COUNT & It calculates the output buffer size of MAP. & User-implemented \\
   %MAP & The map function. & User-implemented \\
   %REDUCE\_COUNT & It calculates the output buffer size of REDUCE. & User-implemented \\
   %REDUCE & The reduce function. & User-implemented \\
   %EMIT\_INTERMEDIATE\_COUNT & It emits the key size and the value size in MAP\_COUNT. & System-provided \\
   %EMIT\_INTERMEDIATE & It emits the key and the value in MAP. & System-provided \\
   %EMIT\_COUNT & It emits the key size and the value size in REDUCE\_COUNT. & System-provided \\
   %EMIT & It emits the key and the value in REDUCE. & System-provided \\
%\noalign{\smallskip}
  %\hline
  %\end{tabular}
  %}}
%\end{table}



\newpage
